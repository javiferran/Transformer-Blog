{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST4Cxm36da_B"
   },
   "source": [
    "<h1><center>The Transformer Blog: fairseq edition</center></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "V3sS4MXSMTUp",
    "outputId": "8e18a8bc-ba0b-49c2-cc0d-112e6eecbfb6"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj0t9YsqL5o5"
   },
   "source": [
    "The Transformer architecture was presented in [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) and introduced a new architecture for many NLP tasks.\n",
    "\n",
    "In this post we present an explanation of the Transformer architecture focusing on the fairseq implementation. We believe this could be useful for researchers and developers working on this framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS9XWk8iL5pN"
   },
   "source": [
    "# Background <a class=\"anchor\" id=\"background\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLocuPnDL5pW"
   },
   "source": [
    "The Transformer was introduced as an alternative model to RNNs and ConvNets that compute representations of its inputs in a constant number of operations. This is made thanks to the self-attention module, which is a kind of attention mechanism that relies on the input sequence of a single data sample to make representations of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juikf9V0L5pX"
   },
   "source": [
    "# Model Architecture <a class=\"anchor\" id=\"model-architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gol0jClL5pY"
   },
   "source": [
    "The Transformer is based on a stack of encoders and another stack of decoders. The encoder maps an input sequence of symbol representations $(x_1, ..., x_n)$ to a sequence of continuous representations $\\mathbf{encoder\\_out} = (encoder\\_out_1, ..., encoder\\_out_n)$. Given $\\mathbf{encoder\\_out}$, the decoder then generates an output sequence $(y_1,...,y_m)$ of symbols one element at a time. At each step the model is auto-regressive [(cite)](https://arxiv.org/abs/1308.0850), consuming the previously generated symbols as additional input when generating the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/transformer_illustrated.png\" width = \"800\" height = \"550\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUIvI-0hWRIK"
   },
   "source": [
    "This model is implemented in fairseq in [fairseq/models/transformer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDcW7a2IL5pt"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(FairseqEncoderDecoderModel):\n",
    "...\n",
    "  def forward(\n",
    "          self,\n",
    "          src_tokens,\n",
    "          src_lengths,\n",
    "          prev_output_tokens,\n",
    "          return_all_hiddens: bool = True,\n",
    "          features_only: bool = False,\n",
    "          alignment_layer: Optional[int] = None,\n",
    "          alignment_heads: Optional[int] = None,\n",
    "      ):\n",
    "          \"\"\"\n",
    "          Run the forward pass for an encoder-decoder model.\n",
    "\n",
    "          Copied from the base class, but without ``**kwargs``,\n",
    "          which are not supported by TorchScript.\n",
    "          \"\"\"\n",
    "          encoder_out = self.encoder(\n",
    "              src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "          )\n",
    "          decoder_out = self.decoder(\n",
    "              prev_output_tokens,\n",
    "              encoder_out=encoder_out,\n",
    "              features_only=features_only,\n",
    "              alignment_layer=alignment_layer,\n",
    "              alignment_heads=alignment_heads,\n",
    "              src_lengths=src_lengths,\n",
    "              return_all_hiddens=return_all_hiddens,\n",
    "          )\n",
    "          return decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvrbtqLpL5po"
   },
   "source": [
    "## Encoder and Decoder Stacks <a class=\"anchor\" id=\"encoder-and-decoder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGcaFv8fC_Vz"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder is composed of a stack of $N=encoder\\_layers$ identical layers. The bottom encoder layer recieves <code class=\"language-plaintext highlighter-rouge\">x = self.forward_embedding(src_tokens, token_embeddings)</code>, which is the (scaled) embedding lookup of shape (batch, src_lenghts, embed_dim) plus a positional embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "biVDRslqtWxX",
    "outputId": "3b5695bf-e830-4b92-c008-2950bcb40522"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/encoder_input.png\" width = \"600\" height = \"200\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2scFenNtXzz"
   },
   "source": [
    "The next encoder layers receive the previous encoder layer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cxN2cGgL5p2"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(FairseqEncoder):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        return_all_hiddens: bool = False,\n",
    "        token_embeddings: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "\n",
    "        x, encoder_embedding = self.forward_embedding(src_tokens, token_embeddings)\n",
    "\n",
    "        # batch x src_lengths x encoder_embed_dim\n",
    "        #                     -> src_lengths x batch x encoder_embed_dim\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # compute padding mask\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n",
    "\n",
    "        encoder_states = [] if return_all_hiddens else None\n",
    "\n",
    "        # encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_padding_mask)\n",
    "            if return_all_hiddens:\n",
    "                assert encoder_states is not None\n",
    "                encoder_states.append(x)\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return EncoderOut(\n",
    "            encoder_out=x,  # src_lengths x batch x encoder_embed_dim\n",
    "            encoder_padding_mask=encoder_padding_mask,\n",
    "            encoder_embedding=encoder_embedding,\n",
    "            encoder_states=encoder_states, # List[src_lengths x batch x encoder_embed_dim]\n",
    "            src_tokens=None,\n",
    "            src_lengths=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK3SS87OsXNc"
   },
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsWQGmeFefPN"
   },
   "source": [
    "The previous snipped of code shows a loop over the layers of the Encoder block <code class=\"language-plaintext highlighter-rouge\">for layer in self.layers</code>. This layer is implemented in fairseq in <code class=\"language-plaintext highlighter-rouge\">class TransformerEncoderLayer(nn.Module)</code> inside [fairseq/modules/transformer_layer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/transformer_layer.py) and computes the following operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "5GmMSgoc025K",
    "outputId": "f648e52e-94f3-4fbe-ecac-462a70c095b6"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/encoder_layer_transformer_full.png\" width = \"400\" height = \"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "razZfUeCtgq8"
   },
   "source": [
    "The input of the encoder layer <code class=\"language-plaintext highlighter-rouge\">x</code> is passed through the self-attention module <code class=\"language-plaintext highlighter-rouge\">self.self_attn</code>, dropout (<code class=\"language-plaintext highlighter-rouge\">self.dropout_module(x)</code>) is then applied before getting to the Add & Normalize module (made of a residual connection <code class=\"language-plaintext highlighter-rouge\">self.residual_connection(x, residual)</code> and a layer normalization (LayerNorm) <code class=\"language-plaintext highlighter-rouge\">self.self_attn_layer_norm(x)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv6olcGSeeBx"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "...\n",
    "  def forward(self, x, encoder_padding_mask, attn_mask: Optional[Tensor] = None):\n",
    "    if attn_mask is not None:\n",
    "      attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)\n",
    "\n",
    "    residual = x\n",
    "    if self.normalize_before:\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "    x, _ = self.self_attn(\n",
    "        query=x,\n",
    "        key=x,\n",
    "        value=x,\n",
    "        key_padding_mask=encoder_padding_mask,\n",
    "        attn_mask=attn_mask,\n",
    "    )\n",
    "    x = self.dropout_module(x)\n",
    "    x = self.residual_connection(x, residual)\n",
    "    if not self.normalize_before:\n",
    "        x = self.self_attn_layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjXHb8wkz00g"
   },
   "source": [
    "Then, the result is passed through a position-wise feed-forward network composed by two fully connected layers, <code class=\"language-plaintext highlighter-rouge\">fc1</code> and <code class=\"language-plaintext highlighter-rouge\">fc2</code> with a ReLU activation in between (<code class=\"language-plaintext highlighter-rouge\">self.activation_fn(self.fc1(x))</code>) and dropout <code class=\"language-plaintext highlighter-rouge\">self.dropout_module(x)</code>.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEzlDkAbz1MX"
   },
   "outputs": [],
   "source": [
    "    residual = x\n",
    "    if self.normalize_before:\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "    x = self.activation_fn(self.fc1(x))\n",
    "    x = self.activation_dropout_module(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.dropout_module(x)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ15CFdI1sMe"
   },
   "source": [
    "Finally, a residual connection is made before another layer normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6Mo8ycF1uEz"
   },
   "outputs": [],
   "source": [
    "    x = self.residual_connection(x, residual)\n",
    "    if not self.normalize_before:\n",
    "        x = self.final_layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WJz_-YHCy98"
   },
   "source": [
    "#### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHKBz0cwoq-d"
   },
   "source": [
    "The input of each encoder layer is firstly passed through a self-attention layer ([fairseq/modules/multihead_attention.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttGbnena7uDT"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key: Optional[Tensor],\n",
    "        value: Optional[Tensor],\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        need_weights: bool = True,\n",
    "        static_kv: bool = False,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        before_softmax: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr6dmsv87Zhx"
   },
   "source": [
    "Each matrix Query (Q), a Key (K), and a Value (V) are created by multiplying the encoder's input by three weight matrices, $W^{Q}, W^{K}$ and $W^{V}$. These $Q$, $K$ and $V$ are matrices in which each row represents the $q$, $k$ and $v$ vector corresponding to each token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz44RSs17W3i"
   },
   "outputs": [],
   "source": [
    "    if self.self_attention:\n",
    "      q = self.q_proj(query)\n",
    "      k = self.k_proj(query)\n",
    "      v = self.v_proj(query)\n",
    "    q *= self.scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW9vxEO28T7R"
   },
   "source": [
    "The self-attention module does the following operation:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\frac{QK^{T}}{d_{k}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb7u2OVM_E2Y"
   },
   "outputs": [],
   "source": [
    "    attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "    assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp_doQCk-Vig"
   },
   "source": [
    "Given a token in the input sequence, $i \\in \\mathcal{X}$, its query vector $q_{i}$ is passed to the self-attention function. Then, a multiple scores are calculated between the query vector $q_{i}$ and the key vectors of the same input sequence $k_{j} \\forall j \\in \\mathcal{X}$. The scores are calculated by means of a dot product. The intuition is that this performs a similarity operation, similar queries and keys vectors will yield higher score.\n",
    "\n",
    "This scores represents how much attention is paid by the self-attention layer to other parts of the sequence when encoding $i$. By multiplying $q_{i}$ by the matrix $K^{T}$, a list of <code class=\"language-plaintext highlighter-rouge\">src_lengths</code> scores is output. The scores are then passed through a softmax layer giving bounded values:\n",
    "\n",
    "$$a_{i} = \\text{softmax}(\\frac{\\mathbf{q}_i {K}^\\top}{\\sqrt{d_k}})\n",
    "= \\frac{\\exp(\\mathbf{q}_i {K}^\\top)}{ \\sqrt{d_k} \\sum_{k_{j} \\in {K}^\\top} \\exp(\\mathbf{q}_i k_{j})}$$\n",
    "\n",
    "where $K^\\top$ is a matrix of key vectors corresponding to the the input sequence $\\mathcal{X}$.\n",
    "\n",
    "For $i=\\text{from}$, its corresponding attention weights $\\alpha_{i}$ could be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5EQf3sC9IlR"
   },
   "source": [
    "The division by the square root of the dimension of the key vectors $d_{k}$ (for getting more stable gradients) is done previously <code class=\"language-plaintext highlighter-rouge\">q *= self.scaling</code> instead in fairseq.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/probs.jpg\" width=\"600\" height = \"400\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmJU_71c8ITP"
   },
   "outputs": [],
   "source": [
    "    attn_weights_float = utils.softmax(\n",
    "                attn_weights, dim=-1, onnx_trace=self.onnx_trace\n",
    "            )\n",
    "    attn_weights = attn_weights_float.type_as(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq6RHyXZozA_"
   },
   "source": [
    "\n",
    "Once we have normalized scores for every pair of tokens $\\{i,j\\}$, we multiply these weights by the value vector $v_{j} \\forall j \\in \\mathcal{X}$ (each row in matrix $V$) and finally sum up those vectors:\n",
    "\n",
    "$$\n",
    "z_{i} = \\sum_{j \\in \\mathcal{X}}\\alpha_{i,j}v_{j}\n",
    "$$\n",
    "\n",
    "Where $z_{i}$ represents row $i$ of $Z$. By doing the matrix of the attention weight matrix <code class=\"language-plaintext highlighter-rouge\">attn_weights</code> and $V$, $\\mathrm{softmax}(\\frac{QK^{T}}{d_{k}})V$, we directly get matrix $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwLvi2-a9vZq"
   },
   "outputs": [],
   "source": [
    "    attn_probs = self.dropout_module(attn_weights)\n",
    "    assert v is not None\n",
    "    attn = torch.bmm(attn_probs, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOj7O92C_3Bg"
   },
   "source": [
    "This process is done in parallel in each of the self-attention heads. So, in total <code class=\"language-plaintext highlighter-rouge\">encoder_attention_heads</code> matrices are output. Each head has its own $W^{Q}, W^{K}$ and $W^{V}$ weight matrices which are randomly initialized, so the result leads to different representation subspaces in each of the self-attention heads.\n",
    "\n",
    "The output matrices $Z$ of every self-attention head are concatenated into a single one to which a linear transformation $W^{O}$ (<code class=\"language-plaintext highlighter-rouge\">self.out_proj</code>) is applied, $$attn = Concat(Z_{head_{i}},\\cdots,Z_{head_{h}})W^{O}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LijfO8SM_TtB"
   },
   "outputs": [],
   "source": [
    "    attn = self.out_proj(attn)\n",
    "    attn_weights: Optional[Tensor] = None\n",
    "    if need_weights:\n",
    "        attn_weights = attn_weights_float.view(\n",
    "            bsz, self.num_heads, tgt_len, src_len\n",
    "        ).transpose(1, 0)\n",
    "        if not need_head_weights:\n",
    "            # average attention weights over heads\n",
    "            attn_weights = attn_weights.mean(dim=0)\n",
    "\n",
    "    return attn, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZtQtirWL5p7"
   },
   "source": [
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=$<code class=\"language-plaintext highlighter-rouge\">encoder_embed_dim</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6uEBO6gL5qF"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is composed of a stack of $N=decoder\\_layers$ identical layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oeKwnH_mVlS"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/transformer_decoding.gif\" width=\"600\" height=\"500\" align=\"center\"\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRwuoPMkL5qG"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(FairseqIncrementalDecoder):\n",
    "  ..\n",
    "  def forward(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        features_only: bool = False,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "        src_lengths: Optional[Any] = None,\n",
    "        return_all_hiddens: bool = False,\n",
    "    ):\n",
    "        \n",
    "    x, extra = self.extract_features(\n",
    "        prev_output_tokens,\n",
    "        encoder_out=encoder_out,\n",
    "        incremental_state=incremental_state,\n",
    "        full_context_alignment=full_context_alignment,\n",
    "        alignment_layer=alignment_layer,\n",
    "        alignment_heads=alignment_heads,\n",
    "    )\n",
    "    if not features_only:\n",
    "        x = self.output_layer(x)\n",
    "    return x, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gd5uLV2WChQJ"
   },
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "    ):\n",
    "    return self.extract_features_scriptable(\n",
    "        prev_output_tokens,\n",
    "        encoder_out,\n",
    "        incremental_state,\n",
    "        full_context_alignment,\n",
    "        alignment_layer,\n",
    "        alignment_heads,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DVizCKiCnjw"
   },
   "outputs": [],
   "source": [
    "def extract_features_scriptable(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "    ):\n",
    "  ..\n",
    "    attn: Optional[Tensor] = None\n",
    "    inner_states: List[Optional[Tensor]] = [x]\n",
    "    for idx, layer in enumerate(self.layers):\n",
    "        if incremental_state is None and not full_context_alignment:\n",
    "            self_attn_mask = self.buffered_future_mask(x)\n",
    "        else:\n",
    "            self_attn_mask = None\n",
    "\n",
    "        x, layer_attn, _ = layer(\n",
    "            x,\n",
    "            encoder_out.encoder_out if encoder_out is not None else None,\n",
    "            encoder_out.encoder_padding_mask if encoder_out is not None else None,\n",
    "            incremental_state,\n",
    "            self_attn_mask=self_attn_mask,\n",
    "            self_attn_padding_mask=self_attn_padding_mask,\n",
    "            need_attn=bool((idx == alignment_layer)),\n",
    "            need_head_weights=bool((idx == alignment_layer)),\n",
    "        )\n",
    "        inner_states.append(x)\n",
    "        if layer_attn is not None and idx == alignment_layer:\n",
    "            attn = layer_attn.float().to(x)\n",
    "\n",
    "    if attn is not None:\n",
    "        if alignment_heads is not None:\n",
    "            attn = attn[:alignment_heads]\n",
    "\n",
    "        # average probabilities over heads\n",
    "        attn = attn.mean(dim=0)\n",
    "\n",
    "    if self.layer_norm is not None:\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "    # T x B x C -> B x T x C\n",
    "    x = x.transpose(0, 1)\n",
    "\n",
    "    if self.project_out_dim is not None:\n",
    "        x = self.project_out_dim(x)\n",
    "\n",
    "    return x, {\"attn\": [attn], \"inner_states\": inner_states}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eMXMKklkQfd"
   },
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFv9Xk6CkX-N"
   },
   "source": [
    "The previous snipped of code shows a loop over the layers of the Decoder block <code class=\"language-plaintext highlighter-rouge\">for idx, layer in enumerate(self.layers):</code>. This layer is implemented in fairseq in <code class=\"language-plaintext highlighter-rouge\">class TransformerDecoderLayer(nn.Module)</code> inside [fairseq/modules/transformer_layer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/transformer_layer.py) and computes the following operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "N3KTQYezkR-e",
    "outputId": "b9beec59-1833-4ba8-eac0-6083d4c02a94"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/Decoder.png\" width = \"300\" height = \"250\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "husHlUjgk1Lo"
   },
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer (Encoder-Decoder Attention), which performs multi-head attention over the output of the encoder stack.  Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "The Transformer Blog (fairseq).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
