{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST4Cxm36da_B"
   },
   "source": [
    "<h1><center>The Transformer Blog: fairseq edition</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj0t9YsqL5o5"
   },
   "source": [
    "The Transformer architecture was presented in [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) and introduced a new architecture for many NLP tasks. In this post we present an explanation of the Transformer architecture focusing on the [fairseq](https://github.com/pytorch/fairseq) implementation. We believe this could be useful for researchers and developers working on this framework.\n",
    "\n",
    "The blog is based on [The annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html), [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) and [Fairseq Transformer, BART](https://yinghaowang.xyz/technology/2020-03-14-FairseqTransformer.html) blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS9XWk8iL5pN"
   },
   "source": [
    "# Background <a class=\"anchor\" id=\"background\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLocuPnDL5pW"
   },
   "source": [
    "The Transformer was introduced as an alternative model to RNNs and ConvNets that compute representations of its inputs in a constant number of operations. This is made thanks to the self-attention module, which is a kind of attention mechanism that relies on the input sequence of a single data sample to make representations of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juikf9V0L5pX"
   },
   "source": [
    "# Model Architecture <a class=\"anchor\" id=\"model-architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gol0jClL5pY"
   },
   "source": [
    "The Transformer is based on a stack of encoders and another stack of decoders. The encoder maps an input sequence of token representations $(x_1, ..., x_{src\\_length})$ to a sequence of continuous representations $\\mathbf{encoder\\_out} = (encoder\\_out_1, ..., encoder\\_out_{src\\_length})$. Given $\\mathbf{encoder\\_out}$, the decoder then generates an output sequence $\\mathcal{Y} = (output_0,...,output_{tgt\\_length})$ of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/transformer_illustrated.png\" style=\"width:650px;height:370px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUIvI-0hWRIK"
   },
   "source": [
    "This model is implemented in fairseq as <code class=\"language-plaintext highlighter-rouge\">TransformerModel</code> in [fairseq/models/transformer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDcW7a2IL5pt"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(FairseqEncoderDecoderModel):\n",
    "...\n",
    "  def forward(\n",
    "          self,\n",
    "          src_tokens,\n",
    "          src_lengths,\n",
    "          prev_output_tokens,\n",
    "          return_all_hiddens: bool = True,\n",
    "          features_only: bool = False,\n",
    "          alignment_layer: Optional[int] = None,\n",
    "          alignment_heads: Optional[int] = None,\n",
    "      ):\n",
    "          \"\"\"\n",
    "          Run the forward pass for an encoder-decoder model.\n",
    "\n",
    "          Copied from the base class, but without ``**kwargs``,\n",
    "          which are not supported by TorchScript.\n",
    "          \"\"\"\n",
    "          encoder_out = self.encoder(\n",
    "              src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "          )\n",
    "          decoder_out = self.decoder(\n",
    "              prev_output_tokens,\n",
    "              encoder_out=encoder_out,\n",
    "              features_only=features_only,\n",
    "              alignment_layer=alignment_layer,\n",
    "              alignment_heads=alignment_heads,\n",
    "              src_lengths=src_lengths,\n",
    "              return_all_hiddens=return_all_hiddens,\n",
    "          )\n",
    "          return decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvrbtqLpL5po"
   },
   "source": [
    "## Encoder and Decoder Stacks <a class=\"anchor\" id=\"encoder-and-decoder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGcaFv8fC_Vz"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder (<code class=\"language-plaintext highlighter-rouge\">TransformerEncoder</code>) is composed of a stack of $N=encoder\\_layers$ identical layers.\n",
    "\n",
    "The encoder recieves a list of tokens $src\\_tokens=(token_{0},...,token_{src\\_len})$ which are then converted to continuous vector representions <code class=\"language-plaintext highlighter-rouge\">x = self.forward_embedding(src_tokens, token_embeddings)</code>, which is the sum of the (scaled) embedding lookup and the positional embedding: <code class=\"language-plaintext highlighter-rouge\">x = embed + self.embed_positions(src_tokens)</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "id": "biVDRslqtWxX",
    "outputId": "3b5695bf-e830-4b92-c008-2950bcb40522"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/encoder_input.png\" style=\"width:480px;height:150px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2scFenNtXzz"
   },
   "source": [
    "From now on, let's consider $\\mathcal{X}^L = (x_{0},\\cdots,x_{src\\_length})$ as the $L$ encoder layer input. $X^{1}$ refers then to the vectors representation of the input sequence tokens of the first layer, after computing <code class=\"language-plaintext highlighter-rouge\">self.forward_embedding</code>. Note that although $\\mathcal{X}^L$ is represented in fairseq as a tensor of shape <code class=\"language-plaintext highlighter-rouge\">src_length x batch x encoder_embed_dim</code>, for the shake of simplicity, we omit the second dimension in the upcoming mathematical notation and just consider it as a <code class=\"language-plaintext highlighter-rouge\">src_length x encoder_embed_dim</code> matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cxN2cGgL5p2"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(FairseqEncoder):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        return_all_hiddens: bool = False,\n",
    "        token_embeddings: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "\n",
    "        x, encoder_embedding = self.forward_embedding(src_tokens, token_embeddings)\n",
    "\n",
    "        # batch x src_lengths x encoder_embed_dim\n",
    "        #                     -> src_lengths x batch x encoder_embed_dim\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # compute padding mask\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n",
    "\n",
    "        encoder_states = [] if return_all_hiddens else None\n",
    "\n",
    "        # encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_padding_mask)\n",
    "            if return_all_hiddens:\n",
    "                assert encoder_states is not None\n",
    "                encoder_states.append(x)\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return EncoderOut(\n",
    "            encoder_out=x,  # src_lengths x batch x encoder_embed_dim\n",
    "            encoder_padding_mask=encoder_padding_mask,\n",
    "            encoder_embedding=encoder_embedding,\n",
    "            encoder_states=encoder_states, # List[src_lengths x batch x encoder_embed_dim]\n",
    "            src_tokens=None,\n",
    "            src_lengths=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a NamedTuple object <code class=\"language-plaintext highlighter-rouge\">encoder_out</code>.\n",
    "\n",
    "* encoder_out: of shape <code class=\"language-plaintext highlighter-rouge\">src_length x batch x encoder_embed_dim</code>, the last layer encoder's embedding which, as we will see, is used by the Decoder. Note that is the same as $\\mathcal{X}^{N+1}$.\n",
    "* encoder_padding_mask: of shape <code class=\"language-plaintext highlighter-rouge\">batch x src_length</code>. Binary ByteTensor where padding elements are indicated by 1.\n",
    "* encoder_embedding: of shape <code class=\"language-plaintext highlighter-rouge\">src_length x batch x encoder_embed_dim</code>, the words (scaled) embedding lookup.\n",
    "* encoder_states: of shape <code class=\"language-plaintext highlighter-rouge\">list[src_length x batch x encoder_embed_dim]</code>, intermediate enocoder layer's output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK3SS87OsXNc"
   },
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsWQGmeFefPN"
   },
   "source": [
    "The previous snipped of code shows a loop over the layers of the Encoder block <code class=\"language-plaintext highlighter-rouge\">for layer in self.layers</code>. This layer is implemented in fairseq in <code class=\"language-plaintext highlighter-rouge\">class TransformerEncoderLayer(nn.Module)</code> inside [fairseq/modules/transformer_layer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/transformer_layer.py) and computes the following operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "5GmMSgoc025K",
    "outputId": "f648e52e-94f3-4fbe-ecac-462a70c095b6"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/encoder_layer_transformer_full.png\" style=\"width:400px;height:350px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "razZfUeCtgq8"
   },
   "source": [
    "The input of the encoder layer is passed through the self-attention module <code class=\"language-plaintext highlighter-rouge\">self.self_attn</code>, dropout (<code class=\"language-plaintext highlighter-rouge\">self.dropout_module(x)</code>) is then applied before getting to the Add & Normalize module (made of a residual connection <code class=\"language-plaintext highlighter-rouge\">self.residual_connection(x, residual)</code> and a layer normalization (LayerNorm) <code class=\"language-plaintext highlighter-rouge\">self.self_attn_layer_norm(x)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv6olcGSeeBx"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "...\n",
    "  def forward(self, x, encoder_padding_mask, attn_mask: Optional[Tensor] = None):\n",
    "    if attn_mask is not None:\n",
    "      attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)\n",
    "\n",
    "    residual = x\n",
    "    if self.normalize_before:\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "    x, _ = self.self_attn(\n",
    "        query=x,\n",
    "        key=x,\n",
    "        value=x,\n",
    "        key_padding_mask=encoder_padding_mask,\n",
    "        attn_mask=attn_mask,\n",
    "    )\n",
    "    x = self.dropout_module(x)\n",
    "    x = self.residual_connection(x, residual)\n",
    "    if not self.normalize_before:\n",
    "        x = self.self_attn_layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjXHb8wkz00g"
   },
   "source": [
    "Then, the result is passed through a position-wise feed-forward network composed by two fully connected layers, <code class=\"language-plaintext highlighter-rouge\">fc1</code> and <code class=\"language-plaintext highlighter-rouge\">fc2</code> with a ReLU activation in between (<code class=\"language-plaintext highlighter-rouge\">self.activation_fn(self.fc1(x))</code>) and dropout <code class=\"language-plaintext highlighter-rouge\">self.dropout_module(x)</code>.\n",
    "\n",
    "$$\\mathrm{Feed Forward}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEzlDkAbz1MX"
   },
   "outputs": [],
   "source": [
    "    residual = x\n",
    "    if self.normalize_before:\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "    x = self.activation_fn(self.fc1(x))\n",
    "    x = self.activation_dropout_module(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.dropout_module(x)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ15CFdI1sMe"
   },
   "source": [
    "Finally, a residual connection is made before another layer normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6Mo8ycF1uEz"
   },
   "outputs": [],
   "source": [
    "    x = self.residual_connection(x, residual)\n",
    "    if not self.normalize_before:\n",
    "        x = self.final_layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WJz_-YHCy98"
   },
   "source": [
    "#### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHKBz0cwoq-d"
   },
   "source": [
    "As we have seen, the input of each encoder layer is firstly passed through a self-attention layer ([fairseq/modules/multihead_attention.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttGbnena7uDT"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key: Optional[Tensor],\n",
    "        value: Optional[Tensor],\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        need_weights: bool = True,\n",
    "        static_kv: bool = False,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        before_softmax: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr6dmsv87Zhx"
   },
   "source": [
    "Each encoder layer input $\\mathcal{X}^L$, shown as <code class=\"language-plaintext highlighter-rouge\">query</code>below since three copies of $query$ are passed to the self-attention module, is multiplied by three weight matrices learned during the training process: $W^{Q}, W^{K}$ and $W^{V}$, obtaining $Q$, $K$ and $V$. Each row of this output matrices represents the query, key and value vectors of each token in the sequence, represented as $q$, $k$ and $v$ in the formulas that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz44RSs17W3i"
   },
   "outputs": [],
   "source": [
    "    if self.self_attention:\n",
    "      q = self.q_proj(query) # Q\n",
    "      k = self.k_proj(query) # K\n",
    "      v = self.v_proj(query) # V\n",
    "    q *= self.scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW9vxEO28T7R"
   },
   "source": [
    "The self-attention module does the following operation:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\frac{QK^\\top}{\\sqrt{d_{k}}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb7u2OVM_E2Y"
   },
   "outputs": [],
   "source": [
    "    attn_weights = torch.bmm(q, k.transpose(1, 2)) # QK^T multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp_doQCk-Vig"
   },
   "source": [
    "Given a token in the input sequence, $i \\in \\mathcal{X}^L$, its query vector $q_{i}$ is passed to the self-attention function. Then, by means of dot products, scalar values (scores) are obtained between the query vector $q_{i}$ and every key vector of the input sequence $k_{j} \\forall j \\in \\mathcal{X}^L$. The intuition is that this performs a similarity operation, similar queries and keys vectors will yield higher scores.\n",
    "\n",
    "This scores represents how much attention is paid by the self-attention layer to other parts of the sequence when encoding $i$. By multiplying $q_{i}$ by the matrix $K^{T}$, a list of <code class=\"language-plaintext highlighter-rouge\">src_length</code> scores is output. The scores are then passed through a softmax function giving bounded values:\n",
    "\n",
    "$$\\alpha_{i} = \\text{softmax}(\\frac{\\mathbf{q}_i {K}^\\top}{\\sqrt{d_k}})\n",
    "= \\frac{\\exp(\\frac{\\mathbf{q}_i {K}^\\top}{\\sqrt{d_k}})}{ \\sum_{j \\in \\mathcal{X}^L} \\exp(\\frac{\\mathbf{q}_i k_{j}^\\top}{\\sqrt{d_k}})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5EQf3sC9IlR"
   },
   "source": [
    "The division by the square root of the dimension of the key vectors $d_{k}$ (for getting more stable gradients) is done previously <code class=\"language-plaintext highlighter-rouge\">q *= self.scaling</code> instead in fairseq.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sentence \"the nice cat walks away from us\" for the token $i=\\text{from}$, its corresponding attention weights $\\alpha_{i}$ for every other token $j$ in the input sequence could be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/probs.jpg\" style=\"width:600px;height:250px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmJU_71c8ITP"
   },
   "outputs": [],
   "source": [
    "    attn_weights_float = utils.softmax(\n",
    "                attn_weights, dim=-1, onnx_trace=self.onnx_trace\n",
    "            )\n",
    "    attn_weights = attn_weights_float.type_as(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq6RHyXZozA_"
   },
   "source": [
    "Once we have normalized scores for every pair of tokens $\\{i,j\\}$, we multiply these weights by the value vector $v_{j} \\forall j \\in \\mathcal{X}$ (each row in matrix $V$) and finally sum up those vectors:\n",
    "\n",
    "$$\n",
    "z_{i} = \\sum_{j \\in \\mathcal{X}}\\alpha_{i,j}v_{j}\n",
    "$$\n",
    "\n",
    "Where $z_{i}$ represents row $i$ of $Z$. By doing the matrix multiplication of the attention weight matrix <code class=\"language-plaintext highlighter-rouge\">attn_weights</code> and $V$, $\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_k}})V$, we directly get matrix $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwLvi2-a9vZq"
   },
   "outputs": [],
   "source": [
    "    attn_probs = self.dropout_module(attn_weights)\n",
    "    assert v is not None\n",
    "    attn = torch.bmm(attn_probs, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOj7O92C_3Bg"
   },
   "source": [
    "This process is done in parallel in each of the self-attention heads. So, in total <code class=\"language-plaintext highlighter-rouge\">encoder_attention_heads</code> matrices are output. Each head has its own $W^{Q}, W^{K}$ and $W^{V}$ weight matrices which are randomly initialized, so the result leads to different representation subspaces in each of the self-attention heads.\n",
    "\n",
    "The output matrices $Z$ of every self-attention head are concatenated into a single one to which a linear transformation $W^{O}$ (<code class=\"language-plaintext highlighter-rouge\">self.out_proj</code>) is applied, $$attn = Concat(Z_{head_{i}},\\cdots,Z_{head_{h}})W^{O}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LijfO8SM_TtB"
   },
   "outputs": [],
   "source": [
    "    attn = self.out_proj(attn)\n",
    "    attn_weights: Optional[Tensor] = None\n",
    "    if need_weights:\n",
    "        attn_weights = attn_weights_float.view(\n",
    "            bsz, self.num_heads, tgt_len, src_len\n",
    "        ).transpose(1, 0)\n",
    "        if not need_head_weights:\n",
    "            # average attention weights over heads\n",
    "            attn_weights = attn_weights.mean(dim=0)\n",
    "\n",
    "    return attn, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that <code class=\"language-plaintext highlighter-rouge\">attn_probs</code> has dimensions (bsz * self.num_heads, tgt_len, src_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZtQtirWL5p7"
   },
   "source": [
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=$<code class=\"language-plaintext highlighter-rouge\">encoder_embed_dim</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6uEBO6gL5qF"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is composed of a stack of $N=decoder\\_layers$ identical layers.\n",
    "\n",
    "The <code class=\"language-plaintext highlighter-rouge\">TransformerDecoder</code> inherits from <code class=\"language-plaintext highlighter-rouge\">FairseqIncrementalDecoder</code>. It differs from the encoder in that it performs incremental decoding. This means that at each time step a forward pass is done through the decoder generating one output token, which is then fed as input to the next time step decoding forward process. Especifically, it takes the encoder's output <code class=\"language-plaintext highlighter-rouge\">encoder_out.encoder_out</code> as $key$ and $value$ matrices (in every decoder layer) and <code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code> to generate one feature vector per target token at each time step (<code class=\"language-plaintext highlighter-rouge\">tgt_len = 1</code> in each forward pass). This feature vector is then passed through a linear layer together with a softmax activation function <code class=\"language-plaintext highlighter-rouge\">self.output_layer(x)</code> to get a probability distribution over the target language vocabulary.\n",
    "\n",
    "Following the beam search algorithm, top <code class=\"language-plaintext highlighter-rouge\">beam</code> hypothesis are chosen and inserted as input of the decoder (<code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code>) for the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oeKwnH_mVlS"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/transformer_decoding.gif\"  style=\"width:750px;height:400px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRwuoPMkL5qG"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(FairseqIncrementalDecoder):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        features_only: bool = False,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "        src_lengths: Optional[Any] = None,\n",
    "        return_all_hiddens: bool = False,\n",
    "    ):\n",
    "        \n",
    "    x, extra = self.extract_features(\n",
    "        prev_output_tokens,\n",
    "        encoder_out=encoder_out,\n",
    "        incremental_state=incremental_state,\n",
    "        full_context_alignment=full_context_alignment,\n",
    "        alignment_layer=alignment_layer,\n",
    "        alignment_heads=alignment_heads,\n",
    "    )\n",
    "    if not features_only:\n",
    "        x = self.output_layer(x)\n",
    "    return x, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gd5uLV2WChQJ"
   },
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "    ):\n",
    "    return self.extract_features_scriptable(\n",
    "        prev_output_tokens,\n",
    "        encoder_out,\n",
    "        incremental_state,\n",
    "        full_context_alignment,\n",
    "        alignment_layer,\n",
    "        alignment_heads,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first time step, <code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code> represents the beginning of sentence (BOS) token index. Its embedding <code class=\"language-plaintext highlighter-rouge\">self.embed_tokens(prev_output_tokens)</code> enters the decoder as a tensor <code class=\"language-plaintext highlighter-rouge\">beam*batch x tgt_len x encoder_embed_dim</code>. As in the decoder, to the input token is added a positional embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DVizCKiCnjw"
   },
   "outputs": [],
   "source": [
    "def extract_features_scriptable(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "    ):\n",
    "  ..\n",
    "    \n",
    "    x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n",
    "    if positions is not None:\n",
    "            x += positions\n",
    "    attn: Optional[Tensor] = None\n",
    "    inner_states: List[Optional[Tensor]] = [x]\n",
    "    for idx, layer in enumerate(self.layers):\n",
    "        if incremental_state is None and not full_context_alignment:\n",
    "            self_attn_mask = self.buffered_future_mask(x)\n",
    "        else:\n",
    "            self_attn_mask = None\n",
    "\n",
    "        x, layer_attn, _ = layer(\n",
    "            x,\n",
    "            encoder_out.encoder_out if encoder_out is not None else None,\n",
    "            encoder_out.encoder_padding_mask if encoder_out is not None else None,\n",
    "            incremental_state,\n",
    "            self_attn_mask=self_attn_mask,\n",
    "            self_attn_padding_mask=self_attn_padding_mask,\n",
    "            need_attn=bool((idx == alignment_layer)),\n",
    "            need_head_weights=bool((idx == alignment_layer)),\n",
    "        )\n",
    "        inner_states.append(x)\n",
    "        if layer_attn is not None and idx == alignment_layer:\n",
    "            attn = layer_attn.float().to(x)\n",
    "\n",
    "    if attn is not None:\n",
    "        if alignment_heads is not None:\n",
    "            attn = attn[:alignment_heads]\n",
    "\n",
    "        # average probabilities over heads\n",
    "        attn = attn.mean(dim=0)\n",
    "\n",
    "    if self.layer_norm is not None:\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "    # T x B x C -> B x T x C\n",
    "    x = x.transpose(0, 1)\n",
    "\n",
    "    if self.project_out_dim is not None:\n",
    "        x = self.project_out_dim(x)\n",
    "\n",
    "    return x, {\"attn\": [attn], \"inner_states\": inner_states}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eMXMKklkQfd"
   },
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFv9Xk6CkX-N"
   },
   "source": [
    "The previous snipped of code shows a loop over the layers of the Decoder block <code class=\"language-plaintext highlighter-rouge\">for idx, layer in enumerate(self.layers):</code>. This layer is implemented in fairseq in <code class=\"language-plaintext highlighter-rouge\">class TransformerDecoderLayer(nn.Module)</code> inside [fairseq/modules/transformer_layer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/transformer_layer.py) and computes the following operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "N3KTQYezkR-e",
    "outputId": "b9beec59-1833-4ba8-eac0-6083d4c02a94"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/Decoder.png\"  style=\"width:350px;height:280px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "husHlUjgk1Lo"
   },
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer (Encoder-Decoder Attention), which performs multi-head attention over the output of the encoder stack as $key$ and $value$ matrices and the ouput of the self-attention module.  Similar to the encoder, it employs residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    ..\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        encoder_out: Optional[torch.Tensor] = None,\n",
    "        encoder_padding_mask: Optional[torch.Tensor] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        prev_self_attn_state: Optional[List[torch.Tensor]] = None,\n",
    "        prev_attn_state: Optional[List[torch.Tensor]] = None,\n",
    "        self_attn_mask: Optional[torch.Tensor] = None,\n",
    "        self_attn_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_attn: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ):\n",
    "\n",
    "        ...\n",
    "        \n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "        if prev_self_attn_state is not None:\n",
    "            prev_key, prev_value = prev_self_attn_state[:2]\n",
    "            saved_state: Dict[str, Optional[Tensor]] = {\n",
    "                \"prev_key\": prev_key,\n",
    "                \"prev_value\": prev_value,\n",
    "            }\n",
    "            if len(prev_self_attn_state) >= 3:\n",
    "                saved_state[\"prev_key_padding_mask\"] = prev_self_attn_state[2]\n",
    "            assert incremental_state is not None\n",
    "            self.self_attn._set_input_buffer(incremental_state, saved_state)\n",
    "        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During incremental decoding, <code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code> $(output_{0},...,output_{t-1})$ enter the self-attention module as key and value vectors. However, only the last time step output token, $output_{t-1}$, enters as a query vector. So, query vectors now have one element in the second dimension, that is, there is no need to use matrix $Q$.\n",
    "As before, scalar values (scores) are obtained between the query vector $q_{t-1}$ and every key vector of the whole previous tokens sequence $k_{j} \\forall j \\in \\mathcal{Y<t}$, where $\\mathcal{Y}$ represents the decoder output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/incremental_decoding.png\"   align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z_{t} = \\sum_{j \\in \\mathcal{Y<t}}\\alpha_{t,j}v_{j}\n",
    "$$\n",
    "\n",
    "Now, just one vector $z_{t}$ is generated at each time step by each head as a weighted average of the $v$ vectors. The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "        y = x\n",
    "\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=y,\n",
    "            value=y,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            incremental_state=incremental_state,\n",
    "            need_weights=False,\n",
    "            attn_mask=self_attn_mask,\n",
    "        )\n",
    "        x = self.dropout_module(x)\n",
    "        x = self.residual_connection(x, residual)\n",
    "        if not self.normalize_before:\n",
    "            x = self.self_attn_layer_norm(x)\n",
    "\n",
    "        if self.encoder_attn is not None and encoder_out is not None:\n",
    "            residual = x\n",
    "            if self.normalize_before:\n",
    "                x = self.encoder_attn_layer_norm(x)\n",
    "            if prev_attn_state is not None:\n",
    "                prev_key, prev_value = prev_attn_state[:2]\n",
    "                saved_state: Dict[str, Optional[Tensor]] = {\n",
    "                    \"prev_key\": prev_key,\n",
    "                    \"prev_value\": prev_value,\n",
    "                }\n",
    "                if len(prev_attn_state) >= 3:\n",
    "                    saved_state[\"prev_key_padding_mask\"] = prev_attn_state[2]\n",
    "                assert incremental_state is not None\n",
    "                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n",
    "\n",
    "            x, attn = self.encoder_attn(\n",
    "                query=x,\n",
    "                key=encoder_out,\n",
    "                value=encoder_out,\n",
    "                key_padding_mask=encoder_padding_mask,\n",
    "                incremental_state=incremental_state,\n",
    "                static_kv=True,\n",
    "                need_weights=need_attn or (not self.training and self.need_attn),\n",
    "                need_head_weights=need_head_weights,\n",
    "            )\n",
    "            x = self.dropout_module(x)\n",
    "            x = self.residual_connection(x, residual)\n",
    "            if not self.normalize_before:\n",
    "                x = self.encoder_attn_layer_norm(x)\n",
    "\n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.final_layer_norm(x)\n",
    "\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.activation_dropout_module(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout_module(x)\n",
    "        x = self.residual_connection(x, residual)\n",
    "        if not self.normalize_before:\n",
    "            x = self.final_layer_norm(x)\n",
    "...\n",
    "        return x, attn, None"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "The Transformer Blog (fairseq).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
