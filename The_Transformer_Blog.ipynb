{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ST4Cxm36da_B"
   },
   "source": [
    "<h1><center>The Transformer Blog: fairseq edition</center></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj0t9YsqL5o5"
   },
   "source": [
    "The Transformer was presented in [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762) and introduced a new architecture for many NLP tasks. In this post we present an explanation of the Transformer architecture on Neural Machine Translation focusing on the [fairseq](https://github.com/pytorch/fairseq) implementation. We believe this could be useful for researchers and developers starting out on this framework.\n",
    "\n",
    "The blog is inspired by [The annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html), [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) and [Fairseq Transformer, BART](https://yinghaowang.xyz/technology/2020-03-14-FairseqTransformer.html) blogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juikf9V0L5pX"
   },
   "source": [
    "# Model Architecture <a class=\"anchor\" id=\"model-architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gol0jClL5pY"
   },
   "source": [
    "The Transformer is based on a stack of encoders and another stack of decoders. The encoder maps an input sequence of tokens $\\mathcal{X}=(token_{0},...,token_{src\\_len})$ to a sequence of continuous vector representations $encoder\\_out = (encoder\\_out_1, ..., encoder\\_out_{src\\_len})$. Given $encoder\\_out$, the decoder then generates an output sequence $\\mathcal{Y} = (output_0,...,output_{T})$ of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/transformer_javifer.png\" style=\"width:600px;height:600px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the general structure of the code in fairseq implementation I recommend reading [Fairseq Transformer, BART](https://yinghaowang.xyz/technology/2020-03-14-FairseqTransformer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUIvI-0hWRIK"
   },
   "source": [
    "This model is implemented in fairseq as <code class=\"language-plaintext highlighter-rouge\">TransformerModel</code> in [fairseq/models/transformer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDcW7a2IL5pt"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(FairseqEncoderDecoderModel):\n",
    "...\n",
    "  def forward(\n",
    "          self,\n",
    "          src_tokens,\n",
    "          src_lengths,\n",
    "          prev_output_tokens,\n",
    "          return_all_hiddens: bool = True,\n",
    "          features_only: bool = False,\n",
    "          alignment_layer: Optional[int] = None,\n",
    "          alignment_heads: Optional[int] = None,\n",
    "      ):\n",
    "          \"\"\"\n",
    "          Run the forward pass for an encoder-decoder model.\n",
    "\n",
    "          Copied from the base class, but without ``**kwargs``,\n",
    "          which are not supported by TorchScript.\n",
    "          \"\"\"\n",
    "          encoder_out = self.encoder(\n",
    "              src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
    "          )\n",
    "          decoder_out = self.decoder(\n",
    "              prev_output_tokens,\n",
    "              encoder_out=encoder_out,\n",
    "              features_only=features_only,\n",
    "              alignment_layer=alignment_layer,\n",
    "              alignment_heads=alignment_heads,\n",
    "              src_lengths=src_lengths,\n",
    "              return_all_hiddens=return_all_hiddens,\n",
    "          )\n",
    "          return decoder_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvrbtqLpL5po"
   },
   "source": [
    "## Encoder and Decoder Stacks <a class=\"anchor\" id=\"encoder-and-decoder\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGcaFv8fC_Vz"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder (<code class=\"language-plaintext highlighter-rouge\">TransformerEncoder</code>) is composed of a stack of $N=encoder\\_layers$ identical layers.\n",
    "\n",
    "The encoder recieves a list of tokens $\\mathcal{X}=$<code class=\"language-plaintext highlighter-rouge\">src_tokens</code>$=(token_{0},...,token_{src\\_len})$ which are then converted to continuous vector representions <code class=\"language-plaintext highlighter-rouge\">x = self.forward_embedding(src_tokens, token_embeddings)</code>, which is made of the sum of the (scaled) embedding lookup and the positional embedding: <code class=\"language-plaintext highlighter-rouge\">x = self.embed_scale * self.embed_tokens(src_tokens) + self.embed_positions(src_tokens)</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2scFenNtXzz"
   },
   "source": [
    "From now on, let's consider $X^L$ as the $L$ encoder layer input sequence. $X^{1}$ refers then to the vectors representation of the input sequence tokens of the first layer, after computing <code class=\"language-plaintext highlighter-rouge\">self.forward_embedding</code> on <code class=\"language-plaintext highlighter-rouge\">src_tokens</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/operations.png\" style=\"width:450px;height:120px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that although $X^L$ is represented in fairseq as a tensor of shape <code class=\"language-plaintext highlighter-rouge\">src_len x batch x encoder_embed_dim</code>, for the shake of simplicity, we take <code class=\"language-plaintext highlighter-rouge\">batch=1</code> in the upcoming mathematical notation and just consider it as a <code class=\"language-plaintext highlighter-rouge\">src_len x encoder_embed_dim</code> matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X^L = \\begin{bmatrix}\n",
    "x_{0}\\\\\n",
    "\\vdots\\\\\n",
    "x_{src\\_len}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Where $x_{i} \\in \\mathbb{R}^{encoder\\_embed\\_dim}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cxN2cGgL5p2"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(FairseqEncoder):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        src_tokens,\n",
    "        src_lengths,\n",
    "        return_all_hiddens: bool = False,\n",
    "        token_embeddings: Optional[torch.Tensor] = None,\n",
    "    ):\n",
    "\n",
    "        x, encoder_embedding = self.forward_embedding(src_tokens, token_embeddings)\n",
    "\n",
    "        # batch x src_lengths x encoder_embed_dim\n",
    "        #                     -> src_lengths x batch x encoder_embed_dim\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # compute padding mask\n",
    "        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n",
    "\n",
    "        encoder_states = [] if return_all_hiddens else None\n",
    "\n",
    "        # encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_padding_mask)\n",
    "            if return_all_hiddens:\n",
    "                assert encoder_states is not None\n",
    "                encoder_states.append(x)\n",
    "\n",
    "        if self.layer_norm is not None:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        return EncoderOut(\n",
    "            encoder_out=x,  # src_lengths x batch x encoder_embed_dim\n",
    "            encoder_padding_mask=encoder_padding_mask,\n",
    "            encoder_embedding=encoder_embedding,\n",
    "            encoder_states=encoder_states, # List[src_lengths x batch x encoder_embed_dim]\n",
    "            src_tokens=None,\n",
    "            src_lengths=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a NamedTuple object <code class=\"language-plaintext highlighter-rouge\">encoder_out</code>.\n",
    "\n",
    "* encoder_out: of shape <code class=\"language-plaintext highlighter-rouge\">src_len x batch x encoder_embed_dim</code>, the last layer encoder's embedding which, as we will see, is used by the Decoder. Note that is the same as $X^{N+1}$ when <code class=\"language-plaintext highlighter-rouge\">batch=1</code>.\n",
    "* encoder_padding_mask: of shape <code class=\"language-plaintext highlighter-rouge\">batch x src_len</code>. Binary ByteTensor where padding elements are indicated by 1.\n",
    "* encoder_embedding: of shape <code class=\"language-plaintext highlighter-rouge\">src_len x batch x encoder_embed_dim</code>, the words (scaled) embedding lookup.\n",
    "* encoder_states: of shape <code class=\"language-plaintext highlighter-rouge\">list[src_len x batch x encoder_embed_dim]</code>, intermediate enocoder layer's output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EK3SS87OsXNc"
   },
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsWQGmeFefPN"
   },
   "source": [
    "The previous snipped of code shows a loop over the layers of the Encoder block, <code class=\"language-plaintext highlighter-rouge\">for layer in self.layers</code>. This layer is implemented in fairseq in <code class=\"language-plaintext highlighter-rouge\">class TransformerEncoderLayer(nn.Module)</code> inside [fairseq/modules/transformer_layer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/transformer_layer.py) and computes the following operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "5GmMSgoc025K",
    "outputId": "f648e52e-94f3-4fbe-ecac-462a70c095b6"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/encoder_javifer.png\" style=\"width:300px;height:400px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "razZfUeCtgq8"
   },
   "source": [
    "The input of the encoder layer is passed through the self-attention module <code class=\"language-plaintext highlighter-rouge\">self.self_attn</code>, dropout (<code class=\"language-plaintext highlighter-rouge\">self.dropout_module(x)</code>) is then applied before getting to the Residual & Normalization module (made of a residual connection <code class=\"language-plaintext highlighter-rouge\">self.residual_connection(x, residual)</code> and a layer normalization (LayerNorm) <code class=\"language-plaintext highlighter-rouge\">self.self_attn_layer_norm(x)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv6olcGSeeBx"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "...\n",
    "  def forward(self, x, encoder_padding_mask, attn_mask: Optional[Tensor] = None):\n",
    "    if attn_mask is not None:\n",
    "      attn_mask = attn_mask.masked_fill(attn_mask.to(torch.bool), -1e8)\n",
    "\n",
    "    residual = x\n",
    "    if self.normalize_before:\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "    x, _ = self.self_attn(\n",
    "        query=x,\n",
    "        key=x,\n",
    "        value=x,\n",
    "        key_padding_mask=encoder_padding_mask,\n",
    "        attn_mask=attn_mask,\n",
    "    )\n",
    "    x = self.dropout_module(x)\n",
    "    x = self.residual_connection(x, residual)\n",
    "    if not self.normalize_before:\n",
    "        x = self.self_attn_layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjXHb8wkz00g"
   },
   "source": [
    "Then, the result is passed through a position-wise feed-forward network composed by two fully connected layers, <code class=\"language-plaintext highlighter-rouge\">fc1</code> and <code class=\"language-plaintext highlighter-rouge\">fc2</code> with a ReLU activation in between (<code class=\"language-plaintext highlighter-rouge\">self.activation_fn(self.fc1(x))</code>) and dropout <code class=\"language-plaintext highlighter-rouge\">self.dropout_module(x)</code>.\n",
    "\n",
    "$$\\text{Feed Forward}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pEzlDkAbz1MX"
   },
   "outputs": [],
   "source": [
    "    residual = x\n",
    "    if self.normalize_before:\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "    x = self.activation_fn(self.fc1(x))\n",
    "    x = self.activation_dropout_module(x)\n",
    "    x = self.fc2(x)\n",
    "    x = self.dropout_module(x)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ15CFdI1sMe"
   },
   "source": [
    "Finally, a residual connection is made before another layer normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6Mo8ycF1uEz"
   },
   "outputs": [],
   "source": [
    "    x = self.residual_connection(x, residual)\n",
    "    if not self.normalize_before:\n",
    "        x = self.final_layer_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WJz_-YHCy98"
   },
   "source": [
    "#### Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHKBz0cwoq-d"
   },
   "source": [
    "As we have seen, the input of each encoder layer is firstly passed through a self-attention layer ([fairseq/modules/multihead_attention.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/multihead_attention.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttGbnena7uDT"
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        query,\n",
    "        key: Optional[Tensor],\n",
    "        value: Optional[Tensor],\n",
    "        key_padding_mask: Optional[Tensor] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        need_weights: bool = True,\n",
    "        static_kv: bool = False,\n",
    "        attn_mask: Optional[Tensor] = None,\n",
    "        before_softmax: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zr6dmsv87Zhx"
   },
   "source": [
    "Each encoder layer input $X^L$, shown as <code class=\"language-plaintext highlighter-rouge\">query</code> below since three identical copies are passed to the self-attention module, is multiplied by three weight matrices learned during the training process: $W^{Q}, W^{K}$ and $W^{V}$, obtaining $Q$, $K$ and $V$. Each row of this output matrices represents the query, key and value vectors of each token in the sequence, represented as $q$, $k$ and $v$ in the formulas that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz44RSs17W3i"
   },
   "outputs": [],
   "source": [
    "    if self.self_attention:\n",
    "      q = self.q_proj(query) # Q\n",
    "      k = self.k_proj(query) # K\n",
    "      v = self.v_proj(query) # V\n",
    "    q *= self.scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW9vxEO28T7R"
   },
   "source": [
    "The self-attention module does the following operation:\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(\\frac{QK^\\top}{\\sqrt{d_{k}}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eb7u2OVM_E2Y"
   },
   "outputs": [],
   "source": [
    "    attn_weights = torch.bmm(q, k.transpose(1, 2)) # QK^T multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp_doQCk-Vig"
   },
   "source": [
    "Given a token in the input sequence, $i \\in X^L$, its query vector $q_{i}$ is passed to the self-attention function. Then, by means of dot products, scalar values (scores) are obtained between the query vector $q_{i}$ and every key vector of the input sequence $k_{j} \\forall j \\in X^L$. The intuition is that this performs a similarity operation, similar queries and keys vectors will yield higher scores.\n",
    "\n",
    "This scores represents how much attention is paid by the self-attention layer to other parts of the sequence when encoding $i$. By multiplying $q_{i}$ by the matrix $K^{T}$, a list of <code class=\"language-plaintext highlighter-rouge\">src_len</code> scores is output. The scores are then passed through a softmax function giving bounded values:\n",
    "\n",
    "$$\\alpha_{i} = \\text{softmax}(\\frac{\\mathbf{q}_i {K}^\\top}{\\sqrt{d_k}})\n",
    "= \\frac{\\exp(\\frac{\\mathbf{q}_i {K}^\\top}{\\sqrt{d_k}})}{ \\sum_{j \\in X^L} \\exp(\\frac{\\mathbf{q}_i k_{j}^\\top}{\\sqrt{d_k}})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmJU_71c8ITP"
   },
   "outputs": [],
   "source": [
    "    attn_weights_float = utils.softmax(\n",
    "                attn_weights, dim=-1, onnx_trace=self.onnx_trace\n",
    "            )\n",
    "    attn_weights = attn_weights_float.type_as(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5EQf3sC9IlR"
   },
   "source": [
    "The division by the square root of the dimension of the key vectors $d_{k}$ (for getting more stable gradients) is done previously <code class=\"language-plaintext highlighter-rouge\">q *= self.scaling</code> instead in fairseq.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given the sentence \"the nice cat walks away from us\" for the token $i=\\text{from}$, its corresponding attention weights $\\alpha_{i}$ for every other token $j$ in the input sequence could be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/probs.jpg\" style=\"width:600px;height:250px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq6RHyXZozA_"
   },
   "source": [
    "Once we have normalized scores for every pair of tokens $\\{i,j\\}$, we multiply these weights by the value vector $v_{j} \\forall j \\in X^L$ (each row in matrix $V$) and finally sum up those vectors:\n",
    "\n",
    "$$\n",
    "z_{i} = \\sum_{j \\in X^L}\\alpha_{i,j}v_{j}\n",
    "$$\n",
    "\n",
    "Where $z_{i}$ represents row $i$ of $Z$. By doing the matrix multiplication of the attention weight matrix <code class=\"language-plaintext highlighter-rouge\">attn_weights</code> and $V$, $\\mathrm{softmax}(\\frac{QK^{T}}{\\sqrt{d_k}})V$, we directly get matrix $Z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwLvi2-a9vZq"
   },
   "outputs": [],
   "source": [
    "    attn_probs = self.dropout_module(attn_weights)\n",
    "    assert v is not None\n",
    "    attn = torch.bmm(attn_probs, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOj7O92C_3Bg"
   },
   "source": [
    "This process is done in parallel in each of the self-attention heads. So, in total <code class=\"language-plaintext highlighter-rouge\">encoder_attention_heads</code> matrices are output. Each head has its own $W^{Q}, W^{K}$ and $W^{V}$ weight matrices which are randomly initialized, so the result leads to different representation subspaces in each of the self-attention heads.\n",
    "\n",
    "The output matrices $Z$ of every self-attention head are concatenated into a single one to which a linear transformation $W^{O}$ (<code class=\"language-plaintext highlighter-rouge\">self.out_proj</code>) is applied, $$attn = Concat(Z_{head_{i}},\\cdots,Z_{head_{h}})W^{O}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LijfO8SM_TtB"
   },
   "outputs": [],
   "source": [
    "    attn = self.out_proj(attn)\n",
    "    attn_weights: Optional[Tensor] = None\n",
    "    if need_weights:\n",
    "        attn_weights = attn_weights_float.view(\n",
    "            bsz, self.num_heads, tgt_len, src_len\n",
    "        ).transpose(1, 0)\n",
    "        if not need_head_weights:\n",
    "            # average attention weights over heads\n",
    "            attn_weights = attn_weights.mean(dim=0)\n",
    "\n",
    "    return attn, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that <code class=\"language-plaintext highlighter-rouge\">attn_probs</code> has dimensions (bsz * self.num_heads, tgt_len, src_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZtQtirWL5p7"
   },
   "source": [
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=$<code class=\"language-plaintext highlighter-rouge\">encoder_embed_dim</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6uEBO6gL5qF"
   },
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is composed of a stack of $N=decoder\\_layers$ identical layers.\n",
    "\n",
    "The goal of the decoder is to generate a sequence $\\mathcal{Y}$ in the target language. The <code class=\"language-plaintext highlighter-rouge\">TransformerDecoder</code> inherits from <code class=\"language-plaintext highlighter-rouge\">FairseqIncrementalDecoder</code>. It differs from the encoder in that it performs incremental decoding. This means that at each time step $t$ a forward pass is done through the decoder, generating $output_{t}$, which is then fed as input to the next time step decoding process.\n",
    "\n",
    "The encoder output <code class=\"language-plaintext highlighter-rouge\">encoder_out.encoder_out</code> is used by the decoder (in each layer) together with $\\mathcal{Y<t}=(output_{0},...,output_{t-1})$ (<code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code>) to generate one feature vector per target token at each time step (<code class=\"language-plaintext highlighter-rouge\">tgt_len = 1</code> in each forward pass). This feature vector is then transformed by a linear layer and passed through a softmax layer <code class=\"language-plaintext highlighter-rouge\">self.output_layer(x)</code> to get a probability distribution over the target language vocabulary.\n",
    "\n",
    "Following the beam search algorithm, top <code class=\"language-plaintext highlighter-rouge\">beam</code> hypothesis are chosen and inserted in the batch dimension input of the decoder (<code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code>) for the next time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oeKwnH_mVlS"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/decoder_javifer.png\" style=\"width:400px;height:500px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider $query^L$ as the $L$ decoder layer input sequence. $query^{1}$ refers then to the vector representation of the input sequence tokens of the first layer, after computing <code class=\"language-plaintext highlighter-rouge\">self.forward_embedding</code> on <code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code>. Note that here <code class=\"language-plaintext highlighter-rouge\">self.forward_embedding</code> is not defined, but we refer to <code class=\"language-plaintext highlighter-rouge\">self.embed_tokens(prev_output_tokens)</code> and <code class=\"language-plaintext highlighter-rouge\">self.embed_positions(prev_output_tokens)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRwuoPMkL5qG"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(FairseqIncrementalDecoder):\n",
    "...\n",
    "  def forward(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        features_only: bool = False,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "        src_lengths: Optional[Any] = None,\n",
    "        return_all_hiddens: bool = False,\n",
    "    ):\n",
    "        \n",
    "    x, extra = self.extract_features(\n",
    "        prev_output_tokens,\n",
    "        encoder_out=encoder_out,\n",
    "        incremental_state=incremental_state,\n",
    "        full_context_alignment=full_context_alignment,\n",
    "        alignment_layer=alignment_layer,\n",
    "        alignment_heads=alignment_heads,\n",
    "    )\n",
    "    if not features_only:\n",
    "        x = self.output_layer(x)\n",
    "    return x, extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gd5uLV2WChQJ"
   },
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "    ):\n",
    "    return self.extract_features_scriptable(\n",
    "        prev_output_tokens,\n",
    "        encoder_out,\n",
    "        incremental_state,\n",
    "        full_context_alignment,\n",
    "        alignment_layer,\n",
    "        alignment_heads,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first time step, <code class=\"language-plaintext highlighter-rouge\">prev_output_tokens</code> represents the beginning of sentence (BOS) token index. Its embedding enters the decoder as a tensor <code class=\"language-plaintext highlighter-rouge\">beam*batch x tgt_len x encoder_embed_dim</code>. As in the case of the encoder, we consider <code class=\"language-plaintext highlighter-rouge\">batch=1</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-DVizCKiCnjw"
   },
   "outputs": [],
   "source": [
    "def extract_features_scriptable(\n",
    "        self,\n",
    "        prev_output_tokens,\n",
    "        encoder_out: Optional[EncoderOut] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        full_context_alignment: bool = False,\n",
    "        alignment_layer: Optional[int] = None,\n",
    "        alignment_heads: Optional[int] = None,\n",
    "    ):\n",
    "  ..\n",
    "    positions = (\n",
    "            self.embed_positions(\n",
    "                prev_output_tokens, incremental_state=incremental_state\n",
    "            )\n",
    "    x = self.embed_scale * self.embed_tokens(prev_output_tokens)\n",
    "    if positions is not None:\n",
    "            x += positions\n",
    "    attn: Optional[Tensor] = None\n",
    "    inner_states: List[Optional[Tensor]] = [x]\n",
    "    for idx, layer in enumerate(self.layers):\n",
    "        if incremental_state is None and not full_context_alignment:\n",
    "            self_attn_mask = self.buffered_future_mask(x)\n",
    "        else:\n",
    "            self_attn_mask = None\n",
    "\n",
    "        x, layer_attn, _ = layer(\n",
    "            x,\n",
    "            encoder_out.encoder_out if encoder_out is not None else None,\n",
    "            encoder_out.encoder_padding_mask if encoder_out is not None else None,\n",
    "            incremental_state,\n",
    "            self_attn_mask=self_attn_mask,\n",
    "            self_attn_padding_mask=self_attn_padding_mask,\n",
    "            need_attn=bool((idx == alignment_layer)),\n",
    "            need_head_weights=bool((idx == alignment_layer)),\n",
    "        )\n",
    "        inner_states.append(x)\n",
    "        if layer_attn is not None and idx == alignment_layer:\n",
    "            attn = layer_attn.float().to(x)\n",
    "\n",
    "    if attn is not None:\n",
    "        if alignment_heads is not None:\n",
    "            attn = attn[:alignment_heads]\n",
    "\n",
    "        # average probabilities over heads\n",
    "        attn = attn.mean(dim=0)\n",
    "\n",
    "    if self.layer_norm is not None:\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "    # T x B x C -> B x T x C\n",
    "    x = x.transpose(0, 1)\n",
    "\n",
    "    if self.project_out_dim is not None:\n",
    "        x = self.project_out_dim(x)\n",
    "\n",
    "    return x, {\"attn\": [attn], \"inner_states\": inner_states}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eMXMKklkQfd"
   },
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFv9Xk6CkX-N"
   },
   "source": [
    "The previous snipped of code shows a loop over the layers of the Decoder block <code class=\"language-plaintext highlighter-rouge\">for idx, layer in enumerate(self.layers):</code>. This layer is implemented in fairseq in <code class=\"language-plaintext highlighter-rouge\">class TransformerDecoderLayer(nn.Module)</code> inside [fairseq/modules/transformer_layer.py](https://github.com/pytorch/fairseq/blob/master/fairseq/modules/transformer_layer.py) and computes the following operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "N3KTQYezkR-e",
    "outputId": "b9beec59-1833-4ba8-eac0-6083d4c02a94"
   },
   "source": [
    "<img src=\"The_Transformer_Blog_files/incremental_decoding.png\" style=\"width:450px;height:550px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "husHlUjgk1Lo"
   },
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer (Encoder-Decoder Attention), which performs multi-head attention over the output of the encoder stack as input for $W^{K}$ and $W^{V}$ and the ouput of the sprevious module $z_{t}*$.  Similar to the encoder, it employs residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    ..\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        encoder_out: Optional[torch.Tensor] = None,\n",
    "        encoder_padding_mask: Optional[torch.Tensor] = None,\n",
    "        incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]] = None,\n",
    "        prev_self_attn_state: Optional[List[torch.Tensor]] = None,\n",
    "        prev_attn_state: Optional[List[torch.Tensor]] = None,\n",
    "        self_attn_mask: Optional[torch.Tensor] = None,\n",
    "        self_attn_padding_mask: Optional[torch.Tensor] = None,\n",
    "        need_attn: bool = False,\n",
    "        need_head_weights: bool = False,\n",
    "    ):\n",
    "\n",
    "        ...\n",
    "        \n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.self_attn_layer_norm(x)      \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-attention in the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During incremental decoding, $(output_{0},...,output_{t-2})$ enter the self-attention module as <code class=\"language-plaintext highlighter-rouge\">prev_key</code> and <code class=\"language-plaintext highlighter-rouge\">prev_value</code> vectors that are stored in <code class=\"language-plaintext highlighter-rouge\">saved_state</code>. Since there is no need to recompute $K$ and $V$ every time, incremental decoding caches these values and concatenates with keys an values from <code class=\"language-plaintext highlighter-rouge\">output_{t-1}</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if prev_self_attn_state is not None:\n",
    "            prev_key, prev_value = prev_self_attn_state[:2]\n",
    "            saved_state: Dict[str, Optional[Tensor]] = {\n",
    "                \"prev_key\": prev_key,\n",
    "                \"prev_value\": prev_value,\n",
    "            }\n",
    "            if len(prev_self_attn_state) >= 3:\n",
    "                saved_state[\"prev_key_padding_mask\"] = prev_self_attn_state[2]\n",
    "            assert incremental_state is not None\n",
    "            self.self_attn._set_input_buffer(incremental_state, saved_state)\n",
    "        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last time step output token, $output_{t-1}$, enters as a query after been embedded. So, queries here have one element in the second dimension, that is, there is no need to use matrix $Q$ notation.\n",
    "As before, scalar values (scores) are obtained between the query vector $q_{t-1}$ and every key vector of the whole previous tokens sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder-Decoder attention receives key and values from the encoder output <code class=\"language-plaintext highlighter-rouge\">encoder_out.encoder_out</code> and the query from the previous module $z_{t}*$. Here, $q_{t}$ is compared against every key vector received from the encoder (and transformed by $W^K$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just one vector $z_{t}$ is generated at each time step by each head as a weighted average of the $v$ vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"The_Transformer_Blog_files/augmented_decoder_javifer.png\" style=\"width:700px;height:400px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "        y = x\n",
    "\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=y,\n",
    "            value=y,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            incremental_state=incremental_state,\n",
    "            need_weights=False,\n",
    "            attn_mask=self_attn_mask,\n",
    "        )\n",
    "        x = self.dropout_module(x)\n",
    "        x = self.residual_connection(x, residual)\n",
    "        if not self.normalize_before:\n",
    "            x = self.self_attn_layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, keys and values calculated from the encoder output are also stored in <code class=\"language-plaintext highlighter-rouge\">saved_state</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if self.encoder_attn is not None and encoder_out is not None:\n",
    "            residual = x\n",
    "            if self.normalize_before:\n",
    "                x = self.encoder_attn_layer_norm(x)\n",
    "            if prev_attn_state is not None:\n",
    "                prev_key, prev_value = prev_attn_state[:2]\n",
    "                saved_state: Dict[str, Optional[Tensor]] = {\n",
    "                    \"prev_key\": prev_key,\n",
    "                    \"prev_value\": prev_value,\n",
    "                }\n",
    "                if len(prev_attn_state) >= 3:\n",
    "                    saved_state[\"prev_key_padding_mask\"] = prev_attn_state[2]\n",
    "                assert incremental_state is not None\n",
    "                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n",
    "\n",
    "            x, attn = self.encoder_attn(\n",
    "                query=x,\n",
    "                key=encoder_out,\n",
    "                value=encoder_out,\n",
    "                key_padding_mask=encoder_padding_mask,\n",
    "                incremental_state=incremental_state,\n",
    "                static_kv=True,\n",
    "                need_weights=need_attn or (not self.training and self.need_attn),\n",
    "                need_head_weights=need_head_weights,\n",
    "            )\n",
    "            x = self.dropout_module(x)\n",
    "            x = self.residual_connection(x, residual)\n",
    "            if not self.normalize_before:\n",
    "                x = self.encoder_attn_layer_norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of the encoder, the result is passed through a position-wise feed-forward network composed by two fully connected layers:\n",
    "\n",
    "$$\\text{Feed Forward}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.final_layer_norm(x)\n",
    "\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = self.activation_dropout_module(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout_module(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a residual connection is made before another layer normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        x = self.residual_connection(x, residual)\n",
    "        if not self.normalize_before:\n",
    "            x = self.final_layer_norm(x)\n",
    "...\n",
    "        return x, attn, None"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "The Transformer Blog (fairseq).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
